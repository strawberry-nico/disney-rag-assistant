import os
import glob
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# å°è¯•å¯¼å…¥ modelscopeï¼Œç”¨äºäº‘ç«¯æé€Ÿä¸‹è½½
try:
    from modelscope.hub.snapshot_download import snapshot_download
except ImportError:
    snapshot_download = None

# --- 1. é…ç½®è·¯å¾„ä¸å‚æ•° ---
PERSIST_DIRECTORY = "chroma_db"
SOURCE_DIRECTORY = "processed_texts"
LOCAL_MODEL_PATH = "./models/bge-m3"  # æœ¬åœ°è·¯å¾„
ONLINE_MODEL_ID = "BAAI/bge-m3"       # çº¿ä¸Š ID

def main():
    # --- 2. ç¡¬ä»¶ä¸æ¨¡å‹è‡ªé€‚åº”åŠ è½½ ---
    # è‡ªåŠ¨æ£€æµ‹ GPU
    use_gpu = torch.cuda.is_available()
    device = "cuda" if use_gpu else "cpu"
    print(f"\n" + "="*40)
    print(f"ğŸ–¥ï¸  æ„å»ºè®¾å¤‡: {device.upper()}")
    
    # æ™ºèƒ½é€‰æ‹©æ¨¡å‹è·¯å¾„
    model_name_or_path = ONLINE_MODEL_ID # é»˜è®¤ç”¨åœ¨çº¿ ID
    
    if os.path.exists(LOCAL_MODEL_PATH):
        print(f"ğŸ“‚ å‘ç°æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH}")
        model_name_or_path = LOCAL_MODEL_PATH
    else:
        print(f"ğŸŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä»äº‘ç«¯åŠ è½½: {ONLINE_MODEL_ID}")
        # å¦‚æœåœ¨ AutoDL (è£…äº† modelscope)ï¼Œåˆ™ä½¿ç”¨æé€Ÿä¸‹è½½
        if snapshot_download:
            try:
                print("ğŸš€ [AutoDL] æ­£åœ¨é€šè¿‡ ModelScope æé€Ÿä¸‹è½½...")
                model_name_or_path = snapshot_download(ONLINE_MODEL_ID)
                print(f"âœ… ä¸‹è½½å®Œæˆï¼Œè·¯å¾„: {model_name_or_path}")
            except Exception as e:
                print(f"âš ï¸ ModelScope ä¸‹è½½å¼‚å¸¸ï¼Œå°è¯•ç›´æ¥åŠ è½½: {e}")

    print(f"ğŸ§  æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (Device={device})...")
    try:
        embedding = HuggingFaceEmbeddings(
            model_name=model_name_or_path,
            model_kwargs={"device": device}, # ğŸ‘ˆ å…³é”®ï¼šè¿™é‡Œæ¢æˆäº† GPU
            encode_kwargs={"normalize_embeddings": True}
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å½»åº•å¤±è´¥: {e}")
        return

    # --- 3. åˆå§‹åŒ–/è¿æ¥å‘é‡åº“ (ä¿ç•™ä½ ä¼˜ç§€çš„å¢é‡é€»è¾‘) ---
    if os.path.exists(PERSIST_DIRECTORY):
        print("ğŸ’¾ æ£€æµ‹åˆ°å·²æœ‰æ•°æ®åº“ï¼Œæ­£åœ¨è¿æ¥...")
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embedding
        )
        # è·å–åº“é‡Œå·²æœ‰çš„æ¥æºæ–‡ä»¶åˆ—è¡¨
        try:
            existing_data = vectorstore.get()
            existing_sources = set()
            if existing_data and 'metadatas' in existing_data:
                for meta in existing_data['metadatas']:
                    if meta and 'source' in meta:
                        existing_sources.add(meta['source'])
            print(f"ğŸ‘€ åº“é‡Œå·²æœ‰ {len(existing_sources)} ä¸ªæ–‡æ¡£ã€‚")
        except Exception:
            existing_sources = set()
    else:
        print("ğŸ†• æœªæ‰¾åˆ°æ•°æ®åº“ï¼Œå°†åˆ›å»ºæ–°åº“...")
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embedding
        )
        existing_sources = set()

    # --- 4. æ‰«æå¹¶è¿‡æ»¤æ–°æ–‡ä»¶ ---
    if not os.path.exists(SOURCE_DIRECTORY):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {SOURCE_DIRECTORY} æ–‡ä»¶å¤¹ï¼è¯·å…ˆä¸Šä¼ æ•°æ®ã€‚")
        return

    all_files = glob.glob(os.path.join(SOURCE_DIRECTORY, "*.txt"))
    new_files = []
    
    for file_path in all_files:
        file_name = os.path.basename(file_path)
        if file_name not in existing_sources:
            new_files.append(file_path)
    
    if not new_files:
        print("âœ… æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†ï¼Œæ•°æ®åº“å·²æ˜¯æœ€æ–°çŠ¶æ€ï¼")
        return

    print(f"ğŸ“¦ å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶ï¼Œå‡†å¤‡å¤„ç†...")

    # --- 5. åŠ è½½ä¸åˆ‡åˆ† ---
    texts = []
    metadatas = []
    
    for file in new_files:
        try:
            with open(file, encoding="utf-8") as f:
                content = f.read()
            if not content.strip(): continue # è·³è¿‡ç©ºæ–‡ä»¶
            texts.append(content)
            metadatas.append({"source": os.path.basename(file)})
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file}: {e}")

    # åˆ‡åˆ†å™¨é…ç½®
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
    )

    new_chunks = []
    new_metadatas = []

    for i, text in enumerate(texts):
        splits = splitter.split_text(text)
        new_chunks.extend(splits)
        new_metadatas.extend([metadatas[i]] * len(splits))

    # --- 6. å†™å…¥æ•°æ®åº“ ---
    if new_chunks:
        print(f"âœ‚ï¸  ç”Ÿæˆäº† {len(new_chunks)} ä¸ªåˆ‡ç‰‡ï¼Œæ­£åœ¨å†™å…¥å‘é‡åº“...")
        vectorstore.add_texts(texts=new_chunks, metadatas=new_metadatas)
        print(f"ğŸ‰ æˆåŠŸæ·»åŠ  {len(new_files)} ä¸ªæ–‡ä»¶åˆ°æ•°æ®åº“ï¼")
        print("="*40 + "\n")
    else:
        print("âš ï¸ æœ‰æ–‡ä»¶ä½†æ²¡åˆ‡åˆ†å‡ºå†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚")

if __name__ == "__main__":
    main()