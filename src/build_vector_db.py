# src/build_vector_db.py
import os
import glob
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

def load_documents():
    texts, metadatas = [], []
    for file in glob.glob("processed_texts/*.txt"):
        with open(file, encoding="utf-8") as f:
            texts.append(f.read())
        metadatas.append({"source": os.path.basename(file)})
    return texts, metadatas

def split_texts(texts, metadatas):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=50,
        separators=[
            "\n\n", "\n", 
            "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "â€¦â€¦",
            "â€", "â€œ",
            " ", ""
        ]
    )
    chunks, chunk_metadatas = [], []
    for i, text in enumerate(texts):
        splits = splitter.split_text(text)
        chunks.extend(splits)
        chunk_metadatas.extend([metadatas[i]] * len(splits))
    return chunks, chunk_metadatas

def main():
    print("ğŸ” åŠ è½½æ–‡æ¡£...")
    texts, metadatas = load_documents()
    if not texts:
        print("âŒ processed_texts/ ä¸ºç©ºï¼è¯·å…ˆè¿è¡Œ parse_docs.py")
        return
    
    print("âœ‚ï¸ åˆ‡åˆ†æ–‡æœ¬...")
    chunks, chunk_metadatas = split_texts(texts, metadatas)
    
    print("ğŸ§  åŠ è½½ BGE-M3ï¼ˆé¦–æ¬¡è¿è¡Œä¼šä¸‹è½½ ï½2.2GBï¼‰...")
    embedding = HuggingFaceBgeEmbeddings(
        model_name="./models/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
        query_instruction="ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"
    )
    
    print("ğŸ’¾ æ„å»º Chroma å‘é‡åº“...")
    vectorstore = Chroma.from_texts(
        texts=chunks,
        embedding=embedding,
        metadatas=chunk_metadatas,
        persist_directory="chroma_db"
    )
    vectorstore.persist()
    print("ğŸ‰ å‘é‡åº“å·²ä¿å­˜åˆ° chroma_db/")

if __name__ == "__main__":
    main()