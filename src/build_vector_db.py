# src/build_vector_db.py
import os
import glob
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# 1. é…ç½®è·¯å¾„
PERSIST_DIRECTORY = "chroma_db"
SOURCE_DIRECTORY = "processed_texts"
MODEL_PATH = "./models/bge-m3"

def main():
    print("ğŸ§  æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
    try:
        embedding = HuggingFaceEmbeddings(
            model_name=MODEL_PATH,
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {e}")
        return

    # 2. åˆå§‹åŒ–/è¿æ¥å‘é‡åº“
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥ from_textsï¼Œè€Œæ˜¯å…ˆè¿æ¥åº“
    if os.path.exists(PERSIST_DIRECTORY):
        print("ğŸ’¾ æ£€æµ‹åˆ°å·²æœ‰æ•°æ®åº“ï¼Œæ­£åœ¨è¿æ¥...")
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embedding
        )
        # è·å–åº“é‡Œå·²æœ‰çš„æ¥æºæ–‡ä»¶åˆ—è¡¨
        try:
            existing_data = vectorstore.get()
            # ä» metadata ä¸­æå– source å­—æ®µï¼Œå»é‡
            existing_sources = set()
            if existing_data and 'metadatas' in existing_data:
                for meta in existing_data['metadatas']:
                    if meta and 'source' in meta:
                        existing_sources.add(meta['source'])
            print(f"ğŸ‘€ åº“é‡Œå·²æœ‰ {len(existing_sources)} ä¸ªæ–‡æ¡£ã€‚")
        except Exception:
            existing_sources = set()
    else:
        print("ğŸ†• æœªæ‰¾åˆ°æ•°æ®åº“ï¼Œå°†åˆ›å»ºæ–°åº“...")
        vectorstore = Chroma(
            persist_directory=PERSIST_DIRECTORY,
            embedding_function=embedding
        )
        existing_sources = set()

    # 3. æ‰«ææœ¬åœ°æ–‡ä»¶å¹¶è¿‡æ»¤
    all_files = glob.glob(os.path.join(SOURCE_DIRECTORY, "*.txt"))
    new_files = []
    
    for file_path in all_files:
        file_name = os.path.basename(file_path)
        # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæ–‡ä»¶åä¸åœ¨åº“é‡Œï¼Œæ‰å¤„ç†
        if file_name not in existing_sources:
            new_files.append(file_path)
    
    if not new_files:
        print("âœ… æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†ï¼Œæ•°æ®åº“å·²æ˜¯æœ€æ–°çŠ¶æ€ï¼")
        return

    print(f"ğŸ“¦ å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶ï¼Œå‡†å¤‡å…¥åº“...")

    # 4. åŠ è½½å¹¶åˆ‡åˆ†æ–°æ–‡ä»¶
    texts = []
    metadatas = []
    
    for file in new_files:
        try:
            with open(file, encoding="utf-8") as f:
                content = f.read()
            texts.append(content)
            metadatas.append({"source": os.path.basename(file)})
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file}: {e}")

    # åˆ‡åˆ†å™¨é…ç½®
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
    )

    new_chunks = []
    new_metadatas = []

    for i, text in enumerate(texts):
        splits = splitter.split_text(text)
        new_chunks.extend(splits)
        # ä¸ºæ¯ä¸ªåˆ‡ç‰‡å¤åˆ¶å¯¹åº”çš„ metadata
        new_metadatas.extend([metadatas[i]] * len(splits))

    # 5. å¢é‡æ·»åŠ åˆ°æ•°æ®åº“
    if new_chunks:
        print(f"âœ‚ï¸  ç”Ÿæˆäº† {len(new_chunks)} ä¸ªæ–°åˆ‡ç‰‡ï¼Œæ­£åœ¨å†™å…¥å‘é‡åº“...")
        # å…³é”®æ–¹æ³•ï¼šadd_texts (è¿½åŠ ) è€Œä¸æ˜¯ from_texts (è¦†ç›–)
        vectorstore.add_texts(texts=new_chunks, metadatas=new_metadatas)
        # Chroma ç°åœ¨çš„ç‰ˆæœ¬é€šå¸¸ä¼šè‡ªåŠ¨ persistï¼Œä½†ä¸ºäº†ä¿é™©å¯ä»¥æ˜¾å¼è°ƒç”¨ï¼ˆè™½ç„¶æ–°ç‰ˆå¯èƒ½å¼ƒç”¨äº†ï¼‰
        # vectorstore.persist() 
        print(f"ğŸ‰ æˆåŠŸæ·»åŠ  {len(new_files)} ä¸ªæ–‡ä»¶åˆ°æ•°æ®åº“ï¼")
    else:
        print("âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–åˆ‡åˆ†å¤±è´¥ã€‚")

if __name__ == "__main__":
    main()