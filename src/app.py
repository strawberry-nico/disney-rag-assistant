import os
import gradio as gr
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings # å¿…é¡»ç”¨è¿™ä¸ªæ–°åº“
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatTongyi
from langchain.prompts import PromptTemplate  # ğŸ‘ˆ æ–°å¢è¿™è¡Œï¼Œç”¨æ¥ç®¡ç†æç¤ºè¯

# 1. æ£€æŸ¥ API Key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    # ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä¸´æ—¶å†™æ­»æµ‹è¯•: DASHSCOPE_API_KEY = "sk-..."
    print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")

# 2. å…¨å±€åŠ è½½ Embedding æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡ï¼ŒèŠ‚çœæ—¶é—´)
print("ğŸ§  æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
try:
    embedding = HuggingFaceEmbeddings(
        model_name="./models/bge-m3",  # æŒ‡å‘æœ¬åœ°æ¨¡å‹è·¯å¾„
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("å°è¯•ä½¿ç”¨ BAAI/bge-m3 åœ¨çº¿æ¨¡å¼...")
    embedding = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )

# 3. å…¨å±€åŠ è½½å‘é‡åº“
if not os.path.exists("chroma_db"):
    raise FileNotFoundError("âŒ æœªæ‰¾åˆ° chroma_db æ–‡ä»¶å¤¹ï¼è¯·å…ˆè¿è¡Œ build_vector_db.py")

print("ğŸ’¾ æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...")
vectorstore = Chroma(
    persist_directory="chroma_db",
    embedding_function=embedding
)

# 4. å…¨å±€å®šä¹‰æ£€ç´¢å™¨ (å…³é”®ï¼ä¹‹å‰æŠ¥é”™å°±æ˜¯å› ä¸ºç¼ºäº†è¿™ä¸ª)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# å®šä¹‰ç±³å¥‡çš„â€œäººè®¾â€æ¨¡æ¿
MICKEY_PROMPT = """
ä½ ç°åœ¨æ˜¯è¿ªå£«å°¼ä¹å›­çš„é‡‘ç‰Œå‘å¯¼â€œç±³å¥‡â€ğŸ­ã€‚
è¯·æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”æ¸¸å®¢çš„é—®é¢˜ã€‚

ğŸ­ **ä½ çš„è¯´è¯è§„åˆ™**ï¼š
1. è¯­æ°”è¦è¶…çº§çƒ­æƒ…ã€å¹½é»˜ï¼Œè¦æŠŠç”¨æˆ·ç§°ä¸ºâ€œäº²çˆ±çš„æœ‹å‹â€æˆ–â€œæ¢é™©å®¶â€ã€‚
2. å¦‚æœèµ„æ–™é‡Œæœ‰ç­”æ¡ˆï¼Œè¯·ç”¨ç”ŸåŠ¨çš„è¯­è¨€æè¿°å‡ºæ¥ï¼Œå¤šç”¨æ„Ÿå¹å·ï¼
3. å¦‚æœèµ„æ–™é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·å§”å©‰åœ°è¯´ï¼šâ€œå“¦ï¼Œè¿™ä¸ªç§˜å¯†è¿ç±³å¥‡ä¹Ÿä¸çŸ¥é“å‘¢ï¼Œæˆ–è®¸æˆ‘ä»¬éœ€è¦å»é—®é—®é­”æ³•å¸ˆï¼â€
4. å›ç­”ç»“æŸæ—¶ï¼Œå¿…é¡»åŠ ä¸Šä¸€å¥è¿ªå£«å°¼çš„ç»å…¸ç¥ç¦ï¼Œæ¯”å¦‚â€œç¥ä½ åœ¨ç¥å¥‡ç‹å›½åº¦è¿‡ç¾å¦™çš„ä¸€å¤©ï¼âœ¨â€ã€‚

ğŸ“– **å‚è€ƒèµ„æ–™**ï¼š
{context}

ğŸ—£ï¸ **æ¸¸å®¢çš„é—®é¢˜**ï¼š
{question}

ç±³å¥‡çš„å›ç­”ï¼š
"""

def rag_answer(query):
    try:
        # 1. æ¯æ¬¡æé—®æ—¶åˆå§‹åŒ– LLM (ä¿æŒä¸å˜)
        llm = ChatTongyi(
            model="qwen-max",
            api_key=DASHSCOPE_API_KEY,
            temperature=0.5 # ç¨å¾®è°ƒé«˜ä¸€ç‚¹ï¼Œè®©ç±³å¥‡è¯´è¯æ›´æœ‰åˆ›é€ åŠ›
        )

        # 2. åˆ›å»ºæç¤ºè¯å¯¹è±¡ (âœ¨é­”æ³•æ ¸å¿ƒåœ¨è¿™é‡Œâœ¨)
        prompt = PromptTemplate(
            template=MICKEY_PROMPT, 
            input_variables=["context", "question"]
        )

        # 3. åˆ›å»ºé—®ç­”é“¾ (æ³¨å…¥ prompt)
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            # ğŸ‘‡ è¿™é‡ŒæŠŠç±³å¥‡çš„äººè®¾ä¼ è¿›å»äº†ï¼
            chain_type_kwargs={"prompt": prompt} 
        )
        
        # 4. æ‰§è¡ŒæŸ¥è¯¢
        print(f"ğŸ” æ¸¸å®¢æé—®: {query}")
        result = qa_chain.invoke({"query": query})
        answer = result["result"]
        
        # 5. æ•´ç†æ¥æº (ä¿æŒä¸å˜)
        seen_sources = set()
        sources_list = []
        if "source_documents" in result:
            for doc in result["source_documents"]:
                src = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡æ¡£'))
                if src not in seen_sources:
                    sources_list.append(f"- {src}")
                    seen_sources.add(src)
        
        sources_str = "\n".join(sources_list) if sources_list else "ç±³å¥‡æ²¡ç¿»åˆ°å°æœ¬æœ¬..."
        return answer, sources_str

    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"âŒ å“å‘€ï¼Œç±³å¥‡çš„é­”æ³•æ£’å¡ä½äº†: {str(e)}", ""

# 6. å¯åŠ¨ Gradio ç•Œé¢
with gr.Blocks(title="è¿ªå£«å°¼RAGåŠ©æ‰‹") as demo:
    gr.Markdown("## ğŸ° è¿ªå£«å°¼ä¹å›­é—®ç­”åŠ©æ‰‹")
    
    with gr.Row():
        with gr.Column():
            input_box = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šé—¨ç¥¨å¤šå°‘é’±ï¼Ÿ")
            submit_btn = gr.Button("ğŸ” æé—®", variant="primary")
        
        with gr.Column():
            output_answer = gr.Textbox(label="AI å›ç­”", lines=6)
            output_sources = gr.Textbox(label="å‚è€ƒæ¥æº", lines=3)
            
    submit_btn.click(
        fn=rag_answer, 
        inputs=input_box, 
        outputs=[output_answer, output_sources]
    )

if __name__ == "__main__":
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­... è¯·åœ¨æµè§ˆå™¨æ‰“å¼€ä¸‹æ–¹é“¾æ¥")
    demo.launch()