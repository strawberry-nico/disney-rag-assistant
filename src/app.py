import os
import json
import time
import torch
import gradio as gr
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatTongyi
from sentence_transformers import CrossEncoder

# å°è¯•å¯¼å…¥ modelscope
try:
    from modelscope.hub.snapshot_download import snapshot_download
except ImportError:
    snapshot_download = None

# --- 1. åŸºç¡€é…ç½® ---
USE_GPU = torch.cuda.is_available()
DEVICE = "cuda" if USE_GPU else "cpu"
ENABLE_RERANK = True if (USE_GPU and snapshot_download) else False

RERANK_MODEL_ID = "BAAI/bge-reranker-v2-m3"
EMBEDDING_MODEL_ID = "BAAI/bge-m3"
PERSIST_DIRECTORY = "chroma_db"
FEEDBACK_FILE = "user_feedback.jsonl"
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    print("âš ï¸ ä¸¥é‡è­¦å‘Š: æœªæ£€æµ‹åˆ° DASHSCOPE_API_KEYï¼")

# --- 2. Prompt ä¸ é€»è¾‘ (ä¿æŒä¸¥è°¨) ---
REWRITE_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æœç´¢å¼•æ“ä¼˜åŒ–åŠ©æ‰‹ã€‚
è¯·å°†ç”¨æˆ·çš„æœç´¢é—®é¢˜é‡å†™ä¸º 3 ä¸ªä¸åŒè§’åº¦çš„æœç´¢å…³é”®è¯ï¼Œä»¥ä¾¿åœ¨å‘é‡æ•°æ®åº“ä¸­æ›´å¥½åœ°æ£€ç´¢ã€‚
åªéœ€è¾“å‡ºå…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–åºŸè¯ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}
é‡å†™ç»“æœï¼š
"""

MICKEY_PROMPT_TEMPLATE = """
ä½ ç°åœ¨æ˜¯è¿ªå£«å°¼ä¹å›­çš„é‡‘ç‰Œå‘å¯¼â€œç±³å¥‡â€ğŸ­ã€‚
è¯·æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”æ¸¸å®¢çš„é—®é¢˜ã€‚
å¦‚æœèµ„æ–™é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·å§”å©‰å‘ŠçŸ¥ã€‚
å›ç­”è¦çƒ­æƒ…ã€å¹½é»˜ï¼Œè¯­æ°”è¦åƒç±³å¥‡ä¸€æ ·æ´»æ³¼ï¼Œæœ€ååŠ ä¸Šä¸€å¥ç¥å¥‡çš„ç¥ç¦ï¼âœ¨

ğŸ“– **å‚è€ƒèµ„æ–™**ï¼š
{context}

ğŸ—£ï¸ **æ¸¸å®¢çš„é—®é¢˜**ï¼š
{question}

ç±³å¥‡çš„å›ç­”ï¼š
"""

# --- 3. åé¦ˆæ•°æ®å­˜å‚¨æ¨¡å— ---
def save_feedback(vote_type, question, answer, sources):
    """ä¿å­˜ç”¨æˆ·åé¦ˆåˆ° JSONL æ–‡ä»¶"""
    if not question or not answer:
        return "âš ï¸ è¿˜æ²¡æœ‰å¯¹è¯å†…å®¹å“¦"
    
    data = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "vote": vote_type,
        "question": question,
        "answer": answer,
        "sources": sources,
        "model_config": "Rerank-v1.2" if ENABLE_RERANK else "CPU-Lite"
    }
    
    try:
        with open(FEEDBACK_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(data, ensure_ascii=False) + "\n")
        return f"âœ… å·²è®°å½•æ‚¨çš„åé¦ˆ ({'ğŸ‘' if vote_type=='up' else 'ğŸ‘'})ï¼Œç±³å¥‡æ”¶åˆ°å•¦ï¼"
    except Exception as e:
        return f"âŒ ä¿å­˜å¤±è´¥: {e}"

# --- 4. æ¨¡å‹åŠ è½½ (ä¿æŒä¸å˜) ---
print(f"ğŸ–¥ï¸  ç¯å¢ƒ: {DEVICE} | Rerank: {ENABLE_RERANK}")
try:
    path = snapshot_download(EMBEDDING_MODEL_ID) if snapshot_download else EMBEDDING_MODEL_ID
    embedding = HuggingFaceEmbeddings(model_name=path, model_kwargs={"device": DEVICE}, encode_kwargs={"normalize_embeddings": True})
except: embedding = None

reranker = None
if ENABLE_RERANK:
    try:
        path = snapshot_download(RERANK_MODEL_ID)
        reranker = CrossEncoder(path, device=DEVICE)
    except: ENABLE_RERANK = False

vectorstore = None
if os.path.exists(PERSIST_DIRECTORY) and embedding:
    vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding)

# --- 5. æ ¸å¿ƒé€»è¾‘ ---
def rag_pipeline(query):
    if not query.strip(): return "", "", "" # å¢åŠ ä¸€ä¸ªç©ºè¿”å›ç»™éšè—çš„state
    
    # Rewrite & Recall
    queries = [query]
    if DASHSCOPE_API_KEY:
        try:
            llm = ChatTongyi(model="qwen-max", api_key=DASHSCOPE_API_KEY)
            res = llm.invoke(REWRITE_PROMPT_TEMPLATE.format(question=query))
            queries.extend([q.strip() for q in res.content.split(',')])
        except: pass
    
    top_k = 50 if ENABLE_RERANK else 3
    candidates = []
    if vectorstore:
        for q in list(set(queries)):
            candidates.extend(vectorstore.similarity_search(q, k=top_k))
    
    # Deduplicate
    unique_docs = {d.page_content: d for d in candidates}
    docs = list(unique_docs.values())

    # Rerank
    if ENABLE_RERANK and reranker and docs:
        pairs = [[query, d.page_content] for d in docs]
        scores = reranker.predict(pairs)
        ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
        docs = [d for d, s in ranked[:3]]
    else:
        docs = docs[:3]
    
    # Generate
    if not docs: return "âŒ æŠ±æ­‰ï¼Œç±³å¥‡æ²¡æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", "", query
    
    context = "\n\n".join([d.page_content for d in docs])
    try:
        llm = ChatTongyi(model="qwen-max", api_key=DASHSCOPE_API_KEY, temperature=0.7)
        resp = llm.invoke(MICKEY_PROMPT_TEMPLATE.format(context=context, question=query))
        answer = resp.content
    except Exception as e: answer = f"âŒ Error: {e}"
    
    sources = "\n".join([f"ğŸ“„ {os.path.basename(d.metadata.get('source','æœªçŸ¥'))}" for d in docs])
    
    # è¿”å›: å›ç­”, æ¥æº, åŸé—®é¢˜(ç”¨äºåé¦ˆ)
    return answer, sources, query

# --- 5. âœ¨ UI ç¾åŒ–é‡æ„åŒº âœ¨ ---

# å®šåˆ¶è¿ªå£«å°¼ä¸»é¢˜ (çº¢è‰²ä¸»è°ƒï¼Œåœ†æ¶¦é£æ ¼)
theme = gr.themes.Soft(
    primary_hue="red",
    secondary_hue="yellow",
    neutral_hue="slate",
    radius_size="lg"
).set(
    button_primary_background_fill="#FF4B4B",
    button_primary_background_fill_hover="#FF2424",
    button_primary_text_color="white",
    block_title_text_color="#FF4B4B"
)

# è‡ªå®šä¹‰ CSS å¢åŠ æ°›å›´æ„Ÿ
css = """
.gradio-container {background-color: #FAFAFA}
h1 {text-align: center; color: #FF4B4B; font-family: 'Comic Sans MS', sans-serif;}
.feedback-btn {font-size: 14px !important;}
"""

with gr.Blocks(theme=theme, css=css, title="Disney RAG Pro") as demo:
    
    # é¡¶éƒ¨æ ‡é¢˜æ 
    gr.HTML("""
    <div style="text-align: center; padding: 20px;">
        <h1 style="font-size: 2.5em; margin-bottom: 10px;">ğŸ° è¿ªå£«å°¼é­”æ³•åŠ©æ‰‹</h1>
        <p style="font-size: 1.2em; color: #666;">
            æˆ‘æ˜¯ç±³å¥‡ ğŸ­ï¼Œä½ çš„ä¸“å±ç§äººå¯¼æ¸¸ï¼(Powered by <b>Rerank</b> & <b>Qwen</b>)
        </p>
    </div>
    """)

    with gr.Row():
        # === å·¦ä¾§ï¼šæ“ä½œåŒº ===
        with gr.Column(scale=4):
            inp = gr.Textbox(
                label="âœ¨ è¯·è¾“å…¥ä½ çš„é—®é¢˜", 
                placeholder="ä¾‹å¦‚ï¼šé‚£ä¸ªéª‘æ‘©æ‰˜è½¦çš„é¡¹ç›®å«ä»€ä¹ˆï¼Ÿ",
                lines=3,
                show_label=True
            )
            
            with gr.Row():
                btn_clear = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                btn_submit = gr.Button("ğŸš€ é­”æ³•æé—®", variant="primary", scale=2)
            
            # å¿«æ·ç¤ºä¾‹ (å½•å±ç¥å™¨ï¼)
            gr.Examples(
                examples=[
                    ["åˆ›æé€Ÿå…‰è½®åˆºæ¿€å—ï¼Ÿ"],
                    ["å¸¦5å²çš„å°å­©å»å“ªé‡Œç©æ¯”è¾ƒå¥½ï¼Ÿ"],
                    ["è¿ªå£«å°¼ä¹å›­å‡ ç‚¹å¼€é—¨ï¼Ÿ"],
                    ["åŠ å‹’æ¯”æµ·ç›—æ’é˜Ÿä¹…å—ï¼Ÿ"]
                ],
                inputs=inp,
                label="ğŸ’¡ è¯•ä¸€è¯•è¿™äº›é—®é¢˜"
            )

        # === å³ä¾§ï¼šå±•ç¤ºåŒº ===
        with gr.Column(scale=5):
            # è¿™é‡Œçš„ state ç”¨äºæš‚å­˜â€œå½“å‰æ­£åœ¨é—®çš„é—®é¢˜â€ï¼Œæ–¹ä¾¿ä¼ ç»™åé¦ˆæŒ‰é’®
            current_question = gr.State()
            
            out_ans = gr.Markdown(label="ç±³å¥‡çš„å›ç­”")
            
            # æ¥æºæŠ˜å èµ·æ¥ï¼Œä¿æŒç•Œé¢æ•´æ´
            with gr.Accordion("ğŸ“š æŸ¥çœ‹çŸ¥è¯†æ¥æº (Rerank Top-3)", open=False):
                out_src = gr.Textbox(label="æ¥æºæ–‡æ¡£", lines=3, show_label=False)
            
            # åé¦ˆåŒº
            with gr.Row():
                gr.Markdown("ğŸ“ **è§‰å¾—è¿™ä¸ªå›ç­”æ€ä¹ˆæ ·ï¼Ÿ**")
                btn_like = gr.Button("ğŸ‘ å¾ˆæœ‰ç”¨", size="sm")
                btn_dislike = gr.Button("ğŸ‘ ä¸å¤ªå‡†", size="sm")
            
            feedback_msg = gr.Markdown(visible=True)

    # --- äº‹ä»¶ç»‘å®š ---
    # æäº¤é—®é¢˜
    btn_submit.click(
        fn=rag_pipeline,
        inputs=inp,
        outputs=[out_ans, out_src, current_question] # åŒæ—¶æ›´æ–°é—®é¢˜åˆ° State
    )
    # å›è½¦æäº¤
    inp.submit(
        fn=rag_pipeline,
        inputs=inp,
        outputs=[out_ans, out_src, current_question]
    )
    # æ¸…ç©º
    btn_clear.click(lambda: ("", "", ""), outputs=[inp, out_ans, out_src])

    # åé¦ˆé€»è¾‘
    btn_like.click(
        fn=lambda q, a: save_feedback("up", q, a, ""),
        inputs=[current_question, out_ans],
        outputs=feedback_msg
    )
    btn_dislike.click(
        fn=lambda q, a: save_feedback("down", q, a, ""),
        inputs=[current_question, out_ans],
        outputs=feedback_msg
    )

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=6006, share=False)