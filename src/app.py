import os
import gradio as gr
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings # å¿…é¡»ç”¨è¿™ä¸ªæ–°åº“
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatTongyi

# 1. æ£€æŸ¥ API Key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    # ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä¸´æ—¶å†™æ­»æµ‹è¯•: DASHSCOPE_API_KEY = "sk-..."
    print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ°ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")

# 2. å…¨å±€åŠ è½½ Embedding æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡ï¼ŒèŠ‚çœæ—¶é—´)
print("ğŸ§  æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹...")
try:
    embedding = HuggingFaceEmbeddings(
        model_name="./models/bge-m3",  # æŒ‡å‘æœ¬åœ°æ¨¡å‹è·¯å¾„
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("å°è¯•ä½¿ç”¨ BAAI/bge-m3 åœ¨çº¿æ¨¡å¼...")
    embedding = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )

# 3. å…¨å±€åŠ è½½å‘é‡åº“
if not os.path.exists("chroma_db"):
    raise FileNotFoundError("âŒ æœªæ‰¾åˆ° chroma_db æ–‡ä»¶å¤¹ï¼è¯·å…ˆè¿è¡Œ build_vector_db.py")

print("ğŸ’¾ æ­£åœ¨è¿æ¥å‘é‡æ•°æ®åº“...")
vectorstore = Chroma(
    persist_directory="chroma_db",
    embedding_function=embedding
)

# 4. å…¨å±€å®šä¹‰æ£€ç´¢å™¨ (å…³é”®ï¼ä¹‹å‰æŠ¥é”™å°±æ˜¯å› ä¸ºç¼ºäº†è¿™ä¸ª)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 5. å®šä¹‰é—®ç­”å‡½æ•°
def rag_answer(query):
    try:
        # âœ… å…³é”®ä¿®æ­£ï¼šæ¯æ¬¡æé—®æ—¶æ‰åˆå§‹åŒ– LLM
        # è¿™èƒ½è§£å†³ "client has been closed" çš„ç½‘ç»œæŠ¥é”™
        llm = ChatTongyi(
            model="qwen-max",
            api_key=DASHSCOPE_API_KEY,
            temperature=0.3
        )

        # åˆ›å»ºé—®ç­”é“¾
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever, # è¿™é‡Œè°ƒç”¨å…¨å±€å®šä¹‰çš„ retriever
            return_source_documents=True
        )
        
        # æ‰§è¡ŒæŸ¥è¯¢
        print(f"ğŸ” ç”¨æˆ·æé—®: {query}")
        result = qa_chain.invoke({"query": query})
        answer = result["result"]
        
        # æ•´ç†æ¥æºæ–‡æ¡£
        seen_sources = set()
        sources_list = []
        if "source_documents" in result:
            for doc in result["source_documents"]:
                src = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥æ–‡æ¡£'))
                if src not in seen_sources:
                    sources_list.append(f"- {src}")
                    seen_sources.add(src)
        
        sources_str = "\n".join(sources_list) if sources_list else "æ— å…·ä½“å‚è€ƒæ–‡æ¡£"
        return answer, sources_str

    except Exception as e:
        # æ‰“å°è¯¦ç»†é”™è¯¯æ–¹ä¾¿è°ƒè¯•
        import traceback
        traceback.print_exc()
        return f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", ""

# 6. å¯åŠ¨ Gradio ç•Œé¢
with gr.Blocks(title="è¿ªå£«å°¼RAGåŠ©æ‰‹") as demo:
    gr.Markdown("## ğŸ° è¿ªå£«å°¼ä¹å›­é—®ç­”åŠ©æ‰‹")
    
    with gr.Row():
        with gr.Column():
            input_box = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šé—¨ç¥¨å¤šå°‘é’±ï¼Ÿ")
            submit_btn = gr.Button("ğŸ” æé—®", variant="primary")
        
        with gr.Column():
            output_answer = gr.Textbox(label="AI å›ç­”", lines=6)
            output_sources = gr.Textbox(label="å‚è€ƒæ¥æº", lines=3)
            
    submit_btn.click(
        fn=rag_answer, 
        inputs=input_box, 
        outputs=[output_answer, output_sources]
    )

if __name__ == "__main__":
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­... è¯·åœ¨æµè§ˆå™¨æ‰“å¼€ä¸‹æ–¹é“¾æ¥")
    demo.launch()